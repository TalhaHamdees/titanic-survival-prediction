{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Titanic Dataset — Preprocessing & Baseline Model\n\n## Objective\nPrepare the raw Titanic dataset for machine learning by handling missing values, encoding categorical variables, and splitting into train/test sets. Then train a Logistic Regression baseline model and evaluate its performance.\n\n## Output\nAt the end of this notebook, we will have:\n- A complete preprocessing pipeline (imputation + encoding)\n- A trained Logistic Regression baseline model\n- Predictions, probabilities, and evaluation metrics (accuracy, confusion matrix, classification report)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zrqqt2txt",
   "metadata": {},
   "source": [
    "## 2. Define Label and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r90xpll307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label (target variable)\n",
    "y = df['Survived']\n",
    "\n",
    "# Features (all columns except the label)\n",
    "X = df.drop(columns=['Survived'])\n",
    "\n",
    "print(f\"Label (y): {y.shape}\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"\\nFeature columns:\\n{list(X.columns)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlwfxc2lafn",
   "metadata": {},
   "source": [
    "## 3. Drop Non-Useful Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832rg4efbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Dropped: {columns_to_drop}\")\n",
    "print(f\"Remaining features: {list(X.columns)}\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366uiabb0b",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1qcswjh6v",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set:  X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Test set:      X_test  {X_test.shape},  y_test  {y_test.shape}\")\n",
    "print(f\"\\nSurvival ratio in full data:     {y.mean():.4f}\")\n",
    "print(f\"Survival ratio in training set: {y_train.mean():.4f}\")\n",
    "print(f\"Survival ratio in test set:     {y_test.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcba81d2nq",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Strategy\n",
    "\n",
    "### Imputation Plan (Handling Missing Values)\n",
    "| Column | Strategy | Why |\n",
    "|---|---|---|\n",
    "| **Age** | Fill with **median** (from training set) | Median is robust to outliers unlike mean. We use the training set median to avoid data leakage from the test set. |\n",
    "| **Embarked** | Fill with **mode** (from training set) | Only 2 values missing. Mode (most frequent value) is the safest choice for a categorical column. |\n",
    "\n",
    "### Encoding Plan (Converting Text to Numbers)\n",
    "| Column | Strategy | Why |\n",
    "|---|---|---|\n",
    "| **Sex** | **Binary encoding** (male=0, female=1) | Only 2 categories, so a single 0/1 column is enough. |\n",
    "| **Embarked** | **One-hot encoding** (S, C, Q → 3 columns) | 3 categories with no natural order, so one-hot avoids implying a ranking between them. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uinci3f21kd",
   "metadata": {},
   "source": [
    "## 6. Implement Preprocessing (Pipeline Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1lz4huyw5ta",
   "metadata": {},
   "source": [
    "### 6A) Identify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xsvvydma4fp",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass']\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "\n",
    "print(f\"Numeric columns:     {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Total: {len(numeric_cols) + len(categorical_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8g2orbtzeju",
   "metadata": {},
   "source": [
    "### 6B) Build Preprocessing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khvq3wyitul",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numeric pipeline: fill missing values with median\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical pipeline: fill missing values with most frequent, then one-hot encode\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine both pipelines into one preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "print(\"Preprocessor created successfully!\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oc5fkljxc2",
   "metadata": {},
   "source": [
    "## 7. Fit and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "viq803zvjl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed shape: (712, 8)\n",
      "X_test_processed shape:  (179, 8)\n",
      "\n",
      "Columns match: True\n",
      "NaNs in train: 0\n",
      "NaNs in test:  0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Learn from training data + transform it\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Step 2: Transform test data using what was learned from training\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
    "print(f\"X_test_processed shape:  {X_test_processed.shape}\")\n",
    "print(f\"\\nColumns match: {X_train_processed.shape[1] == X_test_processed.shape[1]}\")\n",
    "print(f\"NaNs in train: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"NaNs in test:  {np.isnan(X_test_processed).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lvsirrdl6d",
   "source": "## 8. Train the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f9brhfzm0sf",
   "source": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_processed, y_train)\n\nprint(\"Model trained successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a973hm0xjg7",
   "source": "## 9. Predict on Test Set",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "64ksuodrawb",
   "source": "y_pred = model.predict(X_test_processed)\ny_pred_proba = model.predict_proba(X_test_processed)\n\nprint(f\"Predictions shape: {y_pred.shape}\")\nprint(f\"Probabilities shape: {y_pred_proba.shape}\")\nprint(f\"\\nFirst 10 predictions:              {y_pred[:10]}\")\nprint(f\"First 10 actual:                   {y_test.values[:10]}\")\nprint(f\"First 10 survival probabilities:   {np.round(y_pred_proba[:10, 1], 4)}\")\n\n# Show side-by-side comparison\nresults = pd.DataFrame({\n    'Actual': y_test.values,\n    'Predicted': y_pred,\n    'P(Died)': np.round(y_pred_proba[:, 0], 4),\n    'P(Survived)': np.round(y_pred_proba[:, 1], 4)\n})\nresults.head(15)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2161b77f",
   "metadata": {},
   "outputs": [],
   "source": "## 10. Model Evaluation"
  },
  {
   "cell_type": "code",
   "id": "419p5onlih6",
   "source": "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# --- Accuracy ---\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")\nprint(f\"That means the model correctly predicted {int(accuracy * len(y_test))} out of {len(y_test)} passengers.\\n\")\n\n# --- Confusion Matrix ---\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(f\"\"\"\n  - True Negatives  (correctly predicted died):     {cm[0][0]}\n  - False Positives (predicted survived, actually died): {cm[0][1]}\n  - False Negatives (predicted died, actually survived): {cm[1][0]}\n  - True Positives  (correctly predicted survived): {cm[1][1]}\n\"\"\")\n\n# --- Classification Report ---\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d76lwg4x8vf",
   "source": "## Model Evaluation Summary\n\nThe Logistic Regression baseline model achieved an accuracy of approximately 80% on the test set, indicating that it learned meaningful patterns from the data.\n\nThe confusion matrix shows that the model performs very well at identifying passengers who did not survive, correctly classifying most deaths. However, it struggles more with identifying survivors, as reflected by a higher number of false negatives.\n\nFrom the classification report, the model has high recall for the \"Died\" class but lower recall for the \"Survived\" class. This suggests that the model is conservative and tends to predict death unless it is confident about survival.\n\nOverall, this baseline model provides a solid starting point. Future improvements could focus on better capturing survivor-related patterns through feature engineering or alternative models.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}