{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Titanic Dataset — Preprocessing\n",
    "\n",
    "## Objective\n",
    "Prepare the raw Titanic dataset for machine learning by handling missing values, encoding categorical variables, and splitting into train/test sets.\n",
    "\n",
    "## Output\n",
    "At the end of this notebook, we will have:\n",
    "- `X_train`, `X_test` — Feature matrices ready for modeling\n",
    "- `y_train`, `y_test` — Target vectors for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zrqqt2txt",
   "metadata": {},
   "source": [
    "## 2. Define Label and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r90xpll307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label (target variable)\n",
    "y = df['Survived']\n",
    "\n",
    "# Features (all columns except the label)\n",
    "X = df.drop(columns=['Survived'])\n",
    "\n",
    "print(f\"Label (y): {y.shape}\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"\\nFeature columns:\\n{list(X.columns)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlwfxc2lafn",
   "metadata": {},
   "source": [
    "## 3. Drop Non-Useful Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832rg4efbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Dropped: {columns_to_drop}\")\n",
    "print(f\"Remaining features: {list(X.columns)}\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366uiabb0b",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1qcswjh6v",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set:  X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Test set:      X_test  {X_test.shape},  y_test  {y_test.shape}\")\n",
    "print(f\"\\nSurvival ratio in full data:     {y.mean():.4f}\")\n",
    "print(f\"Survival ratio in training set: {y_train.mean():.4f}\")\n",
    "print(f\"Survival ratio in test set:     {y_test.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcba81d2nq",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Strategy\n",
    "\n",
    "### Imputation Plan (Handling Missing Values)\n",
    "| Column | Strategy | Why |\n",
    "|---|---|---|\n",
    "| **Age** | Fill with **median** (from training set) | Median is robust to outliers unlike mean. We use the training set median to avoid data leakage from the test set. |\n",
    "| **Embarked** | Fill with **mode** (from training set) | Only 2 values missing. Mode (most frequent value) is the safest choice for a categorical column. |\n",
    "\n",
    "### Encoding Plan (Converting Text to Numbers)\n",
    "| Column | Strategy | Why |\n",
    "|---|---|---|\n",
    "| **Sex** | **Binary encoding** (male=0, female=1) | Only 2 categories, so a single 0/1 column is enough. |\n",
    "| **Embarked** | **One-hot encoding** (S, C, Q → 3 columns) | 3 categories with no natural order, so one-hot avoids implying a ranking between them. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uinci3f21kd",
   "metadata": {},
   "source": [
    "## 6. Implement Preprocessing (Pipeline Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1lz4huyw5ta",
   "metadata": {},
   "source": [
    "### 6A) Identify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xsvvydma4fp",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass']\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "\n",
    "print(f\"Numeric columns:     {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Total: {len(numeric_cols) + len(categorical_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8g2orbtzeju",
   "metadata": {},
   "source": [
    "### 6B) Build Preprocessing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khvq3wyitul",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numeric pipeline: fill missing values with median\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical pipeline: fill missing values with most frequent, then one-hot encode\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine both pipelines into one preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "print(\"Preprocessor created successfully!\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oc5fkljxc2",
   "metadata": {},
   "source": [
    "## 7. Fit and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "viq803zvjl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed shape: (712, 8)\n",
      "X_test_processed shape:  (179, 8)\n",
      "\n",
      "Columns match: True\n",
      "NaNs in train: 0\n",
      "NaNs in test:  0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Learn from training data + transform it\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Step 2: Transform test data using what was learned from training\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
    "print(f\"X_test_processed shape:  {X_test_processed.shape}\")\n",
    "print(f\"\\nColumns match: {X_train_processed.shape[1] == X_test_processed.shape[1]}\")\n",
    "print(f\"NaNs in train: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"NaNs in test:  {np.isnan(X_test_processed).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161b77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
